import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import os
from typing import List, Dict, Tuple
import re
from numpy.linalg import norm
import pkg_resources

class PolicyChatbot:
    def __init__(self, csv_path: str = None, model_name: str = "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens"):
        """
        ì •ì±… ì±—ë´‡ ì´ˆê¸°í™”
        
        Args:
            csv_path: ì •ì±… ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ
            model_name: ì„ë² ë”© ëª¨ë¸ëª…
        """
        if csv_path is None:
            csv_path = pkg_resources.resource_filename(
                "policy_chatbot_api",
                "data/gyeonggi_smallbiz_policies_2000_ì†Œìƒê³µì¸,ê²½ê¸°_20250705.csv"
            )
        self.csv_path = csv_path
        self.model_name = model_name
        self.data = None
        self.embeddings = None
        self.index = None
        self.model = None
        
        # ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ ì´ˆê¸°í™”
        self._load_data()
        self._initialize_model()
        self._create_embeddings()
        
        # ì§€ì—­ ê³„ì¸µ êµ¬ì¡° ì •ì˜
        self.region_hierarchy = self._get_region_hierarchy()
        
    def _load_data(self):
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            self.data = pd.read_csv(self.csv_path)
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ ì •ì±…")
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            self.data = self.data.fillna("")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            self.data['processed_text'] = self.data.apply(self._preprocess_text, axis=1)
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _preprocess_text(self, row: pd.Series) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ì£¼ìš” í•„ë“œë“¤ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„±
        fields = [
            str(row['title(ê³µê³ ëª…)']),
            str(row['body_text(ê³µê³ ë‚´ìš©)']),
            str(row['ì§€ì›ëŒ€ìƒ']),
            str(row['ì†Œê´€ê¸°ê´€']),
            str(row['ì§€ì›ë¶„ì•¼(ëŒ€)']),
            str(row['ì§€ì›ë¶„ì•¼(ì¤‘)']),
            str(row['ì‚¬ì—…ìˆ˜í–‰ê¸°ê´€']),
            str(row['ë¬¸ì˜ì²˜']),
            str(row['ì‹ ì²­ê¸°ê°„']),
            str(row['ì‚¬ì—…ì‹ ì²­ë°©ë²•ì„¤ëª…'])
        ]
        
        # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
        combined_text = " ".join(fields)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        combined_text = re.sub(r'[^\w\sê°€-í£]', ' ', combined_text)
        combined_text = re.sub(r'\s+', ' ', combined_text).strip()
        
        return combined_text
    
    def _initialize_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = SentenceTransformer(self.model_name)
            print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í•œêµ­ì–´ì— íŠ¹í™”ëœ ëª¨ë¸ë¡œ ëŒ€ì²´
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    def _create_embeddings(self):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            print("ì„ë² ë”© ìƒì„± ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            texts = self.data['processed_text'].tolist()
            self.embeddings = self.model.encode(texts, show_progress_bar=True)
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
            
            # ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´)
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings.astype('float32'))
            
            print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.embeddings)}ê°œ ë²¡í„°")
            
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def search_policies(self, query, top_k=5, similarity_threshold=0.0, region_filter=None, target_filter=None, field_filter=None, region_weight=0.3, target_weight=0.2, field_weight=0.2):
        query_emb = self.model.encode(query)
        query_emb = np.array(query_emb).reshape(1, -1)
        # FAISSì—ì„œ ëª¨ë“  ë²¡í„° ê°€ì ¸ì˜¤ê¸°
        all_embs = self.index.reconstruct_n(0, self.data.shape[0])
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (0~1)
        def cosine_similarity(a, b):
            return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)
        sim_scores = np.array([cosine_similarity(query_emb[0], emb) for emb in all_embs])
        # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ì¸ë±ìŠ¤
        sorted_idx = np.argsort(sim_scores)[::-1]
        results = []
        for idx in sorted_idx:
            row = self.data.iloc[idx]
            # í•˜ë“œ í•„í„° ì ìš© (ì§€ì—­: ì •ì±…ëª…/ë³¸ë¬¸ì— ì§€ì—­ëª… ëª…ì‹œ ì—¬ë¶€ê¹Œì§€ ë°˜ì˜)
            if region_filter:
                org = str(row.get('ì†Œê´€ê¸°ê´€', ''))
                title = str(row.get('title(ê³µê³ ëª…)', ''))
                body = str(row.get('body_text(ê³µê³ ë‚´ìš©)', ''))
                # 1. ì†Œê´€ê¸°ê´€ì´ region_filter(í¬ì²œì‹œ)ë©´ ë¬´ì¡°ê±´ í¬í•¨
                if org == region_filter:
                    pass
                # 2. ì†Œê´€ê¸°ê´€ì´ region_filterì˜ ìƒìœ„(ê²½ê¸°ë„) ë˜ëŠ” ì „êµ­ì´ë©´, title/bodyì— region_filterê°€ ëª…ì‹œë˜ì–´ì•¼ í¬í•¨
                elif region_filter in self.region_hierarchy and org in self.region_hierarchy[region_filter][1:]:
                    if region_filter not in title and region_filter not in body:
                        continue
                # 3. ê·¸ ì™¸(ë‹¤ë¥¸ ì‹œ/êµ°)ëŠ” ì œì™¸
                else:
                    continue
            if target_filter and target_filter not in str(row.get('ì§€ì›ëŒ€ìƒ', '')):
                continue
            if field_filter and field_filter not in str(row.get('ì§€ì›ë¶„ì•¼(ëŒ€)', '')):
                continue
            filter_score = 0.0
            # ì§€ì—­ëª… ê°€ì¤‘ì¹˜ ì œê±° (region_weight ê´€ë ¨ ì½”ë“œ ì‚­ì œ)
            if target_filter:
                filter_score += target_weight
            if field_filter:
                filter_score += field_weight
            final_score = sim_scores[idx] + filter_score
            if final_score >= similarity_threshold:
                results.append({
                    'title': row.get('title(ê³µê³ ëª…)', ''),
                    'body': row.get('body_text(ê³µê³ ë‚´ìš©)', ''),
                    'target': row.get('ì§€ì›ëŒ€ìƒ', ''),
                    'organization': row.get('ì†Œê´€ê¸°ê´€', ''),
                    'field_major': row.get('ì§€ì›ë¶„ì•¼(ëŒ€)', ''),
                    'field_minor': row.get('ì§€ì›ë¶„ì•¼(ì¤‘)', ''),
                    'executing_org': row.get('ì‚¬ì—…ìˆ˜í–‰ê¸°ê´€', ''),
                    'contact': row.get('ë¬¸ì˜ì²˜', ''),
                    'period': row.get('ì‹ ì²­ê¸°ê°„', ''),
                    'application_method': row.get('ì‚¬ì—…ì‹ ì²­ë°©ë²•ì„¤ëª…', ''),
                    'similarity_score': final_score
                })
            if len(results) >= top_k:
                break
        return results
    
    def get_policy_summary(self, query: str) -> str:
        """ì •ì±… ìš”ì•½ ì •ë³´ ìƒì„±"""
        results = self.search_policies(query, top_k=3)
        
        if not results:
            return "ê´€ë ¨ ì •ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        summary = f"'{query}'ì™€ ê´€ë ¨ëœ ì •ì±…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        for result in results:
            summary += f"ğŸ“‹ {result['title']}\n"
            summary += f"ğŸ¯ ì§€ì›ëŒ€ìƒ: {result['target']}\n"
            summary += f"ğŸ¢ ì†Œê´€ê¸°ê´€: {result['organization']}\n"
            summary += f"ğŸ“… ì‹ ì²­ê¸°ê°„: {result['period']}\n"
            summary += f"ğŸ“ ë¬¸ì˜ì²˜: {result['contact']}\n"
            summary += f"ğŸ“ ì‹ ì²­ë°©ë²•: {result['application_method'][:100]}...\n"
            summary += f"ğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {result['similarity_score']:.3f}\n"
            summary += "-" * 50 + "\n"
        
        return summary
    
    def save_model(self, path: str = "policy_chatbot_model.pkl"):
        """ëª¨ë¸ ì €ì¥"""
        try:
            model_data = {
                'data': self.data,
                'embeddings': self.embeddings,
                'index': self.index,
                'model_name': self.model_name
            }
            
            with open(path, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_model(self, path: str = "policy_chatbot_model.pkl"):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with open(path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.data = model_data['data']
            self.embeddings = model_data['embeddings']
            self.index = model_data['index']
            self.model_name = model_data['model_name']
            
            # ëª¨ë¸ ì¬ì´ˆê¸°í™”
            self._initialize_model()
            
            print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = PolicyChatbot()
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_queries = [
        "ì¤‘ì†Œê¸°ì—… ê¸°ìˆ ì§€ì›",
        "ì°½ì—… ì§€ì›",
        "ìˆ˜ì¶œ ì§„ì¶œ",
        "ì²­ë…„ ì§€ì›",
        "AI ê¸°ìˆ  ê°œë°œ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
        print("=" * 50)
        results = chatbot.search_policies(query, top_k=3)
        
        for result in results:
            print(f"ğŸ“‹ {result['title']}")
            print(f"ğŸ¯ {result['target']} | ğŸ“Š ìœ ì‚¬ë„: {result['similarity_score']:.3f}")
            print("-" * 30) 